%%%
%%%  LaTeX template for publications
%%%  to be submitted to Statistical Modelling
%%%
%%%  Prepared by Arnost Komarek
%%%  Version 0.2 (20140214)
%%%    0.2:  style of references slightly changed,
%%%          support for use with bibTeX added
%%%    0.3:  fax removed, orcid added, sections on supplementary material added as well as appendix
\documentclass[submit]{smj}


%%%%% PREAMBLE
%%%%% =============================================================================


%%% Place for putting personal \usepackage and \newcommand commands
%%% Note that some packages are loaded automatically
%%% with the smj class. 
%%% These include: graphicx, color, fancyvrb, amsmath, amssymb, calc, upquote (if available), natbib, url, hyperref.
%%%
%%% Please, specify all your personal definitions, newcommand etc. here
%%% and not inside the main body of the text.
%%% -------------------------------------------------------------------------------
%\usepackage{PACKAGE}
%\newcommand{MYCOMMAND}{...}

\usepackage{floatrow}
\usepackage{float}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsthm}
%\usepackage{natbib}
\usepackage[]{algorithm}
\usepackage{algpseudocode}
\usepackage{array}
\usepackage{threeparttable}
\usepackage{multicol,multirow}
\usepackage{tabularx}
\usepackage{graphicx}
\usepackage{graphics}
\usepackage{makecell}
\usepackage{caption}


%%% Identification of authors
%%% -------------------------------------------------------------------------------
%%% For each author, provide his/her first name, surname and possibly initials 
%%% of the middle names. 
%%%
%%% Use \Affil{NUMBER} following the author name for each unique affiliation,
%%% where NUMBER is integer starting from 1 to the number of affiliations needed
%%% in this paper. In case of multiple affiliations of one author, use
%%% \Affil{NUMBER1,}\Affil{NUMBER2,}\Affil{NUMBER3} following the author's name
%%% as it is done for Emmanuel Lesaffre below.

%%% The ORCID number does not have to be provided. If you want to provide it,
%%% put it in the \ORCID{} command while following (the last) \Affil{} command
%%% of given author as shown below.

  %%% For papers with 3 or more authors:
  %%%  in \Author{}, separate the authors with commas, the last author is separated by `\and' without a comma,
  %%%  in \AuthorRunning{}, use the full name of the first author followed by \textrm{et al.}.

%\Author{Arno\v{s}t Kom\'arek\Affil{1}\ORCID{0000-0001-8778-3762}, 
%        Brian Marx\Affil{2}, 
%        \and Jeffrey S. Simonoff\Affil{3}\ORCID{0000-0002-1496-608X}
%        %and Emmanuel Lesaffre\Affil{4,}\Affil{5}\ORCID{0000-0002-3747-6905}
%}
%\AuthorRunning{Arno\v{s}t Kom\'arek \textrm{et al.}}

  %%% For papers with 2 authors:
  %%%  in both \Author{} and \AuthorRunning{},
  %%%  use the full names of both authors separated by '\and' without a comma.
\Author{Scott H. Koeneman\Affil{1} \and Joseph E. Cavanaugh\Affil{2}}
\AuthorRunning{Scott H. Koeneman \and Joseph E. Cavanuagh}
  
  %%% For papers with 1 author:
  %%%  in both \Author{} and \AuthorRunning{},
  %%%  use the full name the author.
%\Author{Arno\v{s}t Kom\'arek\Affil{1}}
%\AuthorRunning{Arno\v{s}t Kom\'arek}


%%% Affiliations as they should appear on the title page.
%%% -------------------------------------------------------------------------------
%%% Do not provide the full addresses here.
%%% The ordering inside \Affiliations{} should correspond to NUMBERs used 
%%% in \Affil{} commands in \Author{}
\Affiliations{

  %%% 1
\item Division of Biostatistics and Bioinformatics, 
      Department of Pharmacology, Physiology, and Cancer Biology, 
      Thomas Jefferson University,
      Philadelphia,
      PA, USA

  %%% 2
\item Department of Biostatistics,
      The University of Iowa, 
      Iowa City,
      IA, USA

}   %% end \Affiliations


%%% Postal, e-mail address, phone and fax of the corresponding author (not necessarily the first author).
%%% ------------------------------------------------------------------------------------------------------
%%% Use command \CorrAddress{} to provide a full postal address of the
%%% corresponding author in the form
%%% "Firstname Lastname, Department, University, Street 1, ZIP City, Country" 
%%% Use command \CorrEmail{} to provide an e-mail address of the corresponding author.
%%% Use command \CorrPhone{} to provide a phone number (including the country code!) of the corresponding author.
%%% Use command \CorrFax{} to provide a fax number (including the country code!) of the corresponding author.
\CorrAddress{Scott H. Koeneman, 
             Division of Biostatistics and Bioinformatics, 
             Thomas Jefferson University, 
             130 South 9th Street, 
             Philadelphia, PA 55038, 
             USA}
\CorrEmail{Scott.Koeneman@jefferson.edu}
\CorrPhone{(+1)\;651\;792\;6163}


%%% Title and a short title (to be used as a running header) of the paper
%%% -------------------------------------------------------------------------------
\Title{A New Bootstrap Goodness-of-Fit Test for Normal Linear Regression Models}
\TitleRunning{Bootstrap GOF}


%%% Abstract
%%% -------------------------------------------------------------------------------
\Abstract{
In this work, the distributional properties of the goodness-of-fit term in likelihood-based information criteria are explored. These properties are then leveraged
to construct a novel goodness-of-fit test for normal linear regression models that relies on a non-parametric bootstrap. Several simulation studies are performed to
investigate the properties and efficacy of the developed procedure, with these studies demonstrating that the bootstrap test offers distinct advantages as compared
to other methods of assessing the goodness-of-fit of a normal linear regression model. Our inferential technique can be employed using the \texttt{DBModelSelect} R package,
available freely via the Comprehensive R Archive Network.
}


%%% Key words
%%% -------------------------------------------------------------------------------
\Keywords{
Information Criteria; Resampling; Normal Distribution; Information Matrix Test; Robust Variance
}


%%%%% MAIN BODY 
%%%%% =============================================================================
\begin{document}


%%% Title page
%%% -------------------------------------------------------------------------------
%%% Use command\maketitle to produce the title page.

\maketitle


%%% Main text
%%% ------------------------------------------
\section{Introduction}
	\subsection{Goodness-of-Fit}

    		Broadly, the term goodness-of-fit as it pertains to statistical modeling refers to the degree to which a certain model
		and its associated assumptions align with the observed data. The notion of goodness-of-fit is relevant in the model selection realm of information criteria, particularly to the Akaike
		information criterion \citep{Akaike}. Using $\ell(\hat{\theta}|y)$ to denote the log-likelihood of the fitted model and $p$ to denote the
		number of functionally independent estimated parameters, the Akaike information criterion (AIC) for a fitted model can be expressed as
		\begin{equation*}
			AIC = -2 \ell(\hat{\theta}|y) + 2 p.
		\end{equation*}
		The $-2 \ell(\hat{\theta}|y)$ term based on the empirical log-likelihood is known as the goodness-of-fit term as it represents the degree to which the fitted model
		conforms to the observed data $y$ \citep{Cavanaugh}. This term is also present in other likelihood-based information criteria such as the Bayesian information criterion \citep{Schwarz}.
		The goodness-of-fit term will only grow smaller, indicating better conformity to the data, as complexity is added to the model. Thus, this goodness-of-fit term ironically does
		not encompass full goodness-of-fit considerations, and other considerations are needed to avoid overfitting and to detect violations of model assumptions.

		Linear regression analysis imposes a number of assumptions about the data at hand, among them conditional independence and linearity, independence of errors that are normally distributed with a mean of zero,
		and homoskedasticity, that being constant error variance. While this work will focus on assessments of the goodness-of-fit and assumptions of traditional normal linear regression models, it is worth noting
		that assessments of goodness-of-fit exist for many other modeling paradigms as well, and many of the same principles apply.
		
		Residual plots serve as a subjective visual method for assessing the appropriateness of a fitted linear regression model \citep{Miles}. Across different values
		of the predictors and observed values, the residuals should not exhibit any specific pattern other than a mean of zero and constant variance if the assumptions of the linear regression model are met.
		Thus, deviations from this pattern in the form of a residual plot that exhibits curvature, or a change in spread, can indicate violations to linearity and homoskedasticity, respectively. 

		However, this method of visual inspection carries with it certain limitations. When there are a large number of predictors present in a model, it may be impractical to visually inspect every
		possible residual plot with any degree of scrutiny. If heteroskedasticity is induced by covariates that have not been observed, a visual inspection will likely
		not reveal this violation of assumptions.

		Hypothesis tests for the goodness-of-fit of a linear model offer an alternative to visual methods. These tests posit a null hypothesis that the model adequately accommodates the data and that
		the associated assumptions are satisfied, with an alternative that the assumptions are violated in some fundamental way. One such test is the Breusch-Pagan test, which postulates a null hypothesis
		that a linear regression model does not violate homoskedasticity \citep{Breusch}. This test involves performing an auxiliary linear regression on a transformation of the squared residuals from
		the candidate linear regression model against the covariates of interest.

		However, one limitation of the Breusch-Pagan test is that it is only designed to detect a relationship between the covariates and the squared residuals that is linear, and thus it will not
		produce efficacious results if the heteroskedasticity present is not linear \citep{Waldman}. An alternative to the Breusch-Pagan test in this regard is the White test for homoskedasticity,
		which shares the same null hypothesis of homoskedasticity of a linear model as the Breusch-Pagan test \citep{White1980}. The White test produces a test statistic that is sensitive to deviations to
		the null hypothesis in the form of heteroskedasticity related to the squares and cross products of regressors. Additionally, the test may indicate misspecification of the model if the cross products
		of certain regressors should be included in the model, but are not.

		Another type of goodness-of-fit test is found in the information matrix test. The information matrix test compares the differences between the elements of the observed information matrix and the elements
		of the outer product of the score vector, with a null hypothesis that the model is correctly specified \citep{White1982}. This test is more general than the tests for heteroskedasticity, as the null
		hypothesis can be violated in a number of fashions. There are several ways to perform the information matrix test, including varying which elements of the abovementioned matrices to assess, using an 
		auxiliary regression to assist in calculation of the test statistic \citep{Chesher}, and employing a parametric bootstrap to aid in calculation of the test statistic \citep{Dhaene}. While able to
		detect misspecification of various kinds, information matrix tests can struggle with power in certain scenarios.

		The various ways in which goodness-of-fit tests can detect misspecification bring to attention a weakness of the hypothesis test methods as opposed to methods of visual inspection.
		When a hypothesis is rejected, we can be reasonably confident that an assumption is violated; however, by simply performing the test, we do not glean much information as to how exactly the
		model may be misspecified. This is in contrast to the method of visually observing residual plots wherein specific issues may be easier to identify.

		In addition, both the White and Breusch-Pagan tests cannot detect heteroskedasticity induced by unobserved covariates, as their auxiliary regressions only use what has been observed. Such heteroskedasticity
		may not affect the bias and consistency of effect estimates, but may lead to a loss of efficiency of estimates. 

	\subsection*{Robust Variance Estimation}

		When employing likelihood theory to obtain parameter estimates for a model, one can often leverage the properties of maximum likelihood estimators (MLEs) to produce reasonable estimates of the variance of the
		estimators, and thus perform inference \citep{Millar}. However, these properties are only guaranteed to hold when the model is properly specified, and inefficient or biased estimates may result when assumptions
		do not hold. This has given rise to robust variance estimators that rely on fewer assumptions than classical likelihood theory, yet can still quantify the variability of a statistic
		at hand.

		\citet{Huber} first provided justifications for consistency and asymptotic normality of maximum likelihood estimators under conditions weaker than had been previously shown. \citet{White1982}
		expanded upon this notion by deriving covariance matrix estimates for maximum likelihood estimators that are robust to a variety of different types of misspecification and only assume
		conditional independence and certain other regularity conditions. Defining $\theta_*$ as the pseudo-true parameter and $\hat{\theta}_n$ as the MLE for a sample of size
		$n$, White established the asymptotic relation
		\begin{equation*}
			\sqrt{n} (\hat{\theta}_n - \theta_*) \xrightarrow[]{d} N(0, C(\theta_* ) ) .
		\end{equation*}
		The large sample covariance matrix $C(\theta)$ can be defined using two positive definite matrices $A(\theta)$ and $B(\theta)$, where the $(i,j)$ element of $A(\theta)$ is defined as
		\begin{equation*}
			A(\theta)_{i,j} = E \left[ \frac{\partial^2 f(y,\theta)}{\partial \theta_i \partial \theta_j} \right] 
		\end{equation*}
		and the $(i,j)$ element of $B(\theta)$ is defined as
		\begin{equation*}
			B(\theta)_{i,j} = E \left[ \frac{\partial f(y,\theta)}{\partial \theta_i} \frac{\partial f(y,\theta)}{\partial \theta_j} \right] 
		\end{equation*}
		where $i = 1,...,p$ and $j = 1,...,p$ respectively. These matrices then combine to define $C(\theta)$ as
		\begin{equation*}
			C(\theta) = A(\theta)^{-1} B(\theta) A(\theta)^{-1} ,
		\end{equation*}
		and thus evaluating this quantity at $\theta_*$, one arrives at the robust asymptotic variance estimate. The resulting estimator, and those that are similar in form, are often called sandwich
		variance estimators due to one quantity being sandwiched in between two identical others to form the statistic.

		As in general one will not know the pseudo-true parameter $\theta_*$, White formulates the matrices $A_n(\theta)$ and $B_n(\theta)$, with the $(i,j)$ element of $A_n(\theta)$ defined as
		\begin{equation*}
			A_n(\theta)_{i,j} = \frac{1}{n} \sum_{t=1}^{n} \frac{\partial^2 f(y_t,\theta)}{\partial \theta_i \partial \theta_j}
		\end{equation*}
		and the $(i,j)$ element of $B_n(\theta)$ as
		\begin{equation*}
			B_n(\theta)_{i,j} = \frac{1}{n} \sum_{t=1}^{n} \frac{\partial f(y_t,\theta)}{\partial \theta_i} \frac{\partial f(y_t,\theta)}{\partial \theta_j} .
		\end{equation*}
		White proposes calculating $A_n(\theta)$ and $B_n(\theta)$ from the data, and evaluating them at the MLE, to produce
		\begin{equation*}
			C_n(\hat{\theta}) = A_n(\hat{\theta})^{-1} B_n(\hat{\theta}) A_n(\hat{\theta})^{-1} .
		\end{equation*}
		It can then be shown that 
		\begin{equation*}
			C_n(\hat{\theta}_n) \xrightarrow[]{a.s.} C(\theta_* ) .
		\end{equation*}
		Thus, we have a variance estimator for the MLE that is both robust to model misspecification and can be calculated using the data, but yet also will be approximately equivalent to the standard
		likelihood theory estimator if the model is correctly specified. This sandwich estimator, and others like it, can be used to perform inference related to the parameters if one calls into question
		the strong assumptions involved in using the traditional maximum likelihood estimator. However, if these assumptions cannot be met and the model appears to be misspecified, one may ponder
		the merit of performing inference on the pseudo-true parameters in the first place. Therefore, robust variance estimators serve as a hedge against slight deviances from a model
		being correctly specified, not as a tool that remediates poor model selection.

\section{Derivations and Test Formulation}
		
		We will first explore the variance of the log-likelihood goodness-of-fit term present in likelihood-based information criteria. We will assume a
		scenario where a normal linear regression model is being fit to the data of interest, and that this model is not misspecified. Thus, this model is of the proper
		parametric family and contains the requisite mean structure, although the mean structure may contain extraneous variables in the
		case of an overspecified model.

		Assuming a linear model has been fit using maximum likelihood with fitted parameters $\hat{\theta}$, the goodness-of-fit term can be decomposed as
		\begin{equation}
			-2 \ell (\hat{\theta}  ) = n \log(2 \pi) + n + n \log(\hat{\sigma}^2 ) ,
		\end{equation}
		where $\hat{\sigma}^2$ denotes the maximum likelihood estimate for the error variance $\sigma^2$. Note that the only term here that is random is
		the statistic $n \log(\hat{\sigma}^2)$. Thus, if we can quantify the variability of this term, we can quantify the variability of the entire goodness-of-fit
		term.

		To achieve this end, we first consider $\hat{\sigma}^2$. As the linear model is assumed to not be misspecified, and $\hat{\sigma}^2$ is a maximum likelihood
		estimator, this estimator will have an asymptotic variance related to the inverse of the Fisher information \citep{Fisher}. The Fisher information as it relates
		to the parameter vector $\theta' = [\beta', \sigma^2]$, where $\beta$ represents the regression coefficients
		present in the model, can be shown to be
		\begin{equation*}
			- E \left[ \frac{\partial^2 \ell (\hat{\theta}  )}{\partial \theta^2} \right] = \mathcal{I}_{n}(\theta) =
			\begin{bmatrix}
				\frac{X' X}{\sigma^2} & 0 \\
				0 & \frac{n}{2 \sigma^4} \\
			\end{bmatrix}
			,
		\end{equation*}
		where $X$ is the design matrix of the regression, and $0$ is a vector of zeroes. Thus, we may take the inverse of the Fisher information matrix and isolate
		the element related to the error variance $\sigma^2$. We see that this element will be
		\begin{equation*}
			\mathcal{I}^{-1}(\sigma ^2) = \frac{2 \sigma ^4}{n} .
		\end{equation*}
		
		Using the above relation, and applying the property of asymptotic normality of the MLE $\hat{\sigma}^2$ in this case of a properly specified normal linear
		regression model, we see that the asymptotic distribution
		\begin{equation*}
			\sqrt{n} (\hat{\sigma}^2 - \sigma^2) \xrightarrow[]{d} N(0, 2 \sigma ^4 )
		\end{equation*}
		should hold.

		To find the variance of $-2 \ell (\hat{\theta}  )$, we must find the variance of $n \log(\hat{\sigma}^2)$.
		Additionally, the above asymptotic distribution involves the true $\sigma^2$ to which we will not have access in practical scenarios.

		We will address both of these issues by employing the delta method \citep{Rao}. We propose a transformation of the form
		\begin{equation*}
			g(x) = \log(x) .
		\end{equation*}
		Thus, applying the delta method to our above asymptotic distribution with $g(x)$ as the function of interest, we see that
		\begin{equation*}
			\sqrt{n} ( \log (\hat{\sigma}^2) - \log(\sigma^2)) \xrightarrow[]{d} N(0, 2) .
		\end{equation*}

		Armed with the above asymptotic relationship and assuming that this asymptotic property approximately holds in a setting with a finite $n$, we have that
		\begin{equation*}
			n\log(\hat{\sigma}^2) \; \dot\sim \; N \left( n\log(\sigma^2), 2n \right) .
		\end{equation*}
		Thus, assuming that the model is appropriately specified, the variance of $n\log(\hat{\sigma}^2)$ will be approximately $2n$. Applying this variance back to the goodness-of-fit term,
		we see that the approximation
		\begin{equation*}
			Var \left[ -2 \ell (\hat{\theta}  ) \right] \approx 2n
		\end{equation*}
		is justified. Furthermore, $2n$ could also serve as an approximation to the variance of AIC or BIC for this correctly specified linear regression model. This approximation
		has the same form no matter the complexity of the design matrix $X$ or value of the true parameters $\theta$, making it useful as a
		general tool.

		It should be noted that this approximation does rely on asymptotic properties. In the Appendix, an exact variance for $-2 \ell (\hat{\theta})$ is found which does
		not rely on asymptotic properties. However, this variance is more complicated to compute than the simple approximation $2n$, and was not found to provide any meaningful
		benefit over $2n$ when used in procedures developed later in this work.

		With the previous derivation in hand, we now develop an estimator for the variance of $n\log(\hat{\sigma}^2)$, and thus $-2 \ell (\hat{\theta})$,
		that need not assume a given fitted normal linear regression model is correctly specified.

		Assume that we once again fit a linear regression model with parameters $\theta' = [\beta', \sigma^2]$ to our data, and suppose we do not know whether this model is correctly specified.
		We wish to construct an estimator for $Var \left[ -2 \ell (\hat{\theta}  ) \right]$ that is robust to model misspecification. This estimator will be constructed using the White robust
		sandwich variance estimator, employing much of the notation related to this development that was introduced in the first section of this work \citep{White1980}.

		We let $I_{n} (\theta)$ denote the observed information matrix with regards to our specified linear regression model. With $X$ denoting the $n$ by $r$ design matrix, we see that
		the quantity $A_n (\theta)$ used in the White estimator is found to be 
		\begin{equation*}
			A_n(\theta) = \frac{1}{n}
			\begin{bmatrix}
				\frac{X'X}{\sigma^2} & \left[ \frac{-(y-X\beta)'X}{\sigma^4} \right]' \\
				\left[ \frac{-(y-X\beta)'X}{\sigma^4} \right] &  \frac{n}{2 \sigma^4} - \frac{(y-X\beta)'(y-X\beta)}{\sigma^6}
				\end{bmatrix}
				= -\frac{1}{n} I_n(\theta) .
		\end{equation*}
		Now let $x_i$ be the $i$th row of the design matrix $X$, and $y_i$ be the $i$th observation of the observation vector $y$. With these constructs at hand, we can define the score components
		for individual observations in our sample as
		\begin{equation*}
			U_i(\theta) = 
			\begin{bmatrix}
				\frac{(y_i-x_i \beta)x_i'}{\sigma^2} \\
				\frac{-1}{2 \sigma^2} + \frac{(y_i - x_i \beta)^2}{2 \sigma^4}
			\end{bmatrix}
			.
		\end{equation*}
		With these components defined, we can then use the preceding to represent the matrix $B_n (\theta)$ used in the White estimator as
		\begin{equation*}
			B_n(\theta) = \frac{1}{n} \sum_{i=1}^{n} U_i(\theta) U_i(\theta)' .
		\end{equation*}

		Thus, with $B_n(\theta)$ and $A_n(\theta)$ defined, these matrices can be evaluated at the maximum likelihood estimator $\hat{\theta}$ and be used to define
		\begin{equation*}
			\begin{split}
				C_n(\hat{\theta}) & = A^{-1}_n(\hat{\theta}) B_n(\hat{\theta}) A^{-1}_n(\hat{\theta}) \\
				& = n I_n^{-1}(\hat{\theta}) \left[ \sum_{i=1}^{n} U_i(\hat{\theta}) U_i(\hat{\theta})' \right] I_n^{-1}(\hat{\theta}) ,
			\end{split}
		\end{equation*}
		where $C_n(\hat{\theta})$ will be a $(r+1)$ by $(r+1)$ matrix that can be used as an estimator of the asymptotic variance-covariance matrix of $\hat{\theta}$. This estimator is robust to misspecification
		of the model.
		
		Consider the bottom rightmost element of this matrix, that being the $(r+1)^{th}$ element of the $(r+1)^{th}$ column of the matrix. This element will correspond to the large-sample
		robust variance of $\hat{\sigma}^2$. Let $s(\theta)$ refer to this corresponding element in the case of the theoretical matrix $C(\theta)$, and $s_n(\hat{\theta})$ refer to this
		corresponding element of the estimator $C_n(\hat{\theta})$. By White's results, it can then be seen that
		\begin{equation}
			\sqrt{n} (\hat{\sigma}^2 - \sigma_*^2) \xrightarrow[]{d} N(0, s(\theta_*)) ,
		\end{equation}
		where $\sigma_*^2$ denotes the pseudo-true parameter related to $\sigma^2$ in the case of potential misspecification.

		We will again employ the delta method to transform the asymptotic distribution to a form that is more suitable. We once more define a transformation of
		\begin{equation*}
			g(x) = \log(x) .
		\end{equation*}
		Applying this transformation to the asymptotic distribution presented in (2.2), we arrive at the relation
		\begin{equation*}
			\sqrt{n} ( \log (\hat{\sigma}^2) - \log(\sigma_*^2)) \xrightarrow[]{d} N \left( 0, \frac{1}{\sigma_*^4} s(\theta_*) \right) .
		\end{equation*}
		Using this asymptotic distribution, one can arrive at an approximate distribution for $n\log(\hat{\sigma}^2)$ as
		\begin{equation*}
			n\log(\hat{\sigma}^2) \; \dot\sim \; N \left( n\log(\sigma_* ^2), \frac{n}{\sigma_*^4} s(\theta_*) \right) ,
		\end{equation*}
		which could be suitable for use in finite sample sizes that are sufficiently large. However, $\sigma_*^2$ and $s(\theta_*)$ are unlikely to be unknown in practical modeling
		applications. Thus, estimating these quantities with $\hat{\sigma}^2$ and $s_n(\hat{\theta})$ respectively, a reasonable estimate for the variance of $n\log(\hat{\sigma}^2)$ can
		be proposed as
		\begin{equation*}
			Var \left[ n\log(\hat{\sigma}^2) \right] \approx \frac{n}{\hat{\sigma}^4} s_n(\hat{\theta}) .
		\end{equation*}
		By the relation presented in (2.1), it is clear that this variance estimate is also suitable for $Var \left[ -2 \ell (\hat{\theta}  ) \right]$, and therefore
		likelihood-based information criteria as a whole that possess a constant penalty term. This estimator should approximate our previously derived value $2n$ in the case of a correctly
		specified model, as the sandwich estimator component will approximate the expected Fisher information used earlier, and the MLE $\hat{\sigma}^2$ should converge to the true
		parameter value $\sigma^2$. However, this sandwich estimator need not assume correct specification, and should be relatively robust to model misspecification.
		
		For the remainder of this work, this sandwich estimator will be referred to as $\widehat{Var}[GOF]$, such that
		\begin{equation*}
			\widehat{Var}[GOF] = \frac{n}{\hat{\sigma}^4} s_n(\hat{\theta}) .
		\end{equation*}

		We have established an asymptotic variance for the likelihood goodness-of-fit term in the case of a correctly specified normal linear regression model, and an
		estimator for this variance that does not assume the model is correctly specified. We will now synthesize these two developments to form a general goodness-of-fit procedure to
		test the hypothesis that a given normal linear regression model is correctly specified.

		Under the null hypothesis that a normal linear regression model is correctly specified, the estimator $\widehat{Var}[GOF]$ should be close to the theoretical value $2n$ for a
		sufficient sample size. We propose the use of the non-parametric bootstrap to obtain an empirical estimate for the sampling distribution of $\widehat{Var}[GOF]$. Once this empirical distribution
		has been obtained, the null hypothesis can be tested by observing whether a bootstrap interval for $Var[GOF]$ contains the theoretical value $2n$, as the approximation
		$Var[GOF] \approx 2n$ should hold for sufficient sample sizes under the null hypothesis. If a $100*(1-\alpha)$\% bootstrap confidence interval does not contain 
		$2n$, we reject the null hypothesis and conclude that the model is misspecified; however, if the interval does contain $2n$, we do not have sufficient evidence to reject
		the null hypothesis, and the proposed model does not demonstrate lack-of-fit. A full summary of the proposed procedure can be found in the presented Algorithm.
		%\pagebreak
		\begin{algorithm*}[h]
			\caption*{$\bf{Algorithm}$ Bootstrap Goodness-of-Fit Test for a Normal Linear Regression Model}
			\begin{algorithmic}[1]
			  \Statex For test level $\alpha$, candidate normal linear model $M$, bootstrap iterations $B$, sample size $n$, and a null hypothesis that $M$
			  is adequately specified:
			  \State Resample, with replacement, outcomes with covariates to generate a bootstrap sample of size $n$.
			  \State Fit model $M$ to this bootstrap sample, and with this fitted model, calculate $\widehat{Var}[GOF]$
			  and record this statistic.
			  \State Repeat steps 1-2 $B$ times to generate an empirical bootstrap distribution for $\widehat{Var}[GOF]$.
			  \State Construct a $100*(1-\alpha)$\% bootstrap confidence interval for $Var[GOF]$.
			  \State If this interval does not contain $2n$, reject the null hypothesis at the $\alpha$ level. If it does contain
			  $2n$, the null hypothesis was not rejected and model $M$ does not appear to exhibit lack-of-fit. 
			\end{algorithmic}
		\end{algorithm*}

		This procedure can be used to assess the general hypothesis that a normal linear regression model is properly specified against the alternative that it displays lack-of-fit.
		Unlike other goodness-of-fit tests and procedures, this method does not test a specific assumption of linear regression such as normality or homoskedasticity, but rather
		many assumptions of normal linear regression. If some assumptions are violated, the property $Var[GOF] \approx 2n$ is not guaranteed to hold. This characteristic allows the test to detect
		many forms of misspecification from mean misspecification to distributional misspecification to heteroskedasticity. 

		Additionally, an implementation of this bootstrap test was developed in the R package \texttt{DBModelSelect} \citep{Koeneman}, available for download via CRAN and Github.
		The \texttt{BootGOFTestLM} function in this package provides a convenient and efficient way to perform the bootstrap test on a fitted linear model.
		
		While the simulations in the following section will employ a percentile interval as the bootstrap confidence interval method of choice, one is not limited to using this method
		and may use any bootstrap interval they choose so long as it is theoretically justifiable. Additionally, the non-parametric bootstrap is employed in the algorithm detailed above to avoid further
		assumptions. Limited testing results suggest that the residual bootstrap may also work just as well in this procedure. However, pending further investigation, the non-parametric
		bootstrap is recommended at the current time. All simulations presented in this work will use the non-parametric bootstrap implementation of the procedure.

\section{Simulation Studies}

Four simulation settings will be employed to assess the efficacy of the bootstrap procedure developed in the previous section. In each simulation, an $n$ by 1 outcome vector $y = [y_1,...,y_n]'$ will
be generated for $i = 1,...,n$ according to
\begin{equation*}
	y_i = 2.0 + 2.0 x_{i1} + 2.0 x_{i2} + \epsilon_i , 
\end{equation*}
with the nature of the error term $\epsilon$ and the fitted candidate model varying between different simulations.

In the first simulation scenario, each $\epsilon_i$ will be generated as $\epsilon_i \stackrel{iid}{\sim} N(0,4)$, and $x_{i1}$ and $x_{i2}$ are completely $iid$ covariates generated according to
a $Uniform(0,5)$ distribution. With the generated data in hand, we will proceed to fit a normal linear regression model that includes an intercept and effects for $x_{i1}$ and $x_{i2}$. This model
is properly specified, and should not exhibit gross lack-of-fit.

In the second simulation scenario, once again each $\epsilon_i$ will be generated as $\epsilon_i \stackrel{iid}{\sim} N(0,4)$, and $x_{i1}$ and $x_{i2}$ are completely $iid$ covariates generated according to
a $Uniform(0,5)$ distribution. However, in this case the fitted model will be a normal linear regression model with an intercept and effect for only
$x_{i1}$. The covariate $x_{i2}$ will be omitted from the model, and could be viewed as a factor that affects the true generating process, but
was unobserved in data collection. This omission will induce mean misspecification for this fitted model as the mean structure will be underspecified.

In the third simulation scenario, each $\epsilon_i$ will be generated as $\epsilon_i \stackrel{iid}{\sim} N(0,4)$, and $x_{i1}$ and $x_{i2}$ are completely $iid$ covariates generated according to
a $Uniform(0,5)$ distribution. However, we will generate a third variable $x_{i3}$ such that $x_{i3} = 0.3 x_{i2} + 0.7 U$, where $U$ is drawn according to a $Uniform(0,5)$ distribution.
Then, the fitted model will be a normal linear regression model with an intercept and effects for $x_{i1}$ and $x_{i3}$. This model is misspecified as while the covariate $x_{i3}$ 
may serve as a surrogate for $x_{i2}$ due to their relation, we are still missing information used in the true generating process.

In the fourth simulation scenario, each $\epsilon_i$ will be generated as $\epsilon_i \stackrel{iid}{\sim} N \left( 0,(2 + x_{i3})^2 \right)$, and $x_{i1}$, $x_{i2}$, and $x_{i3}$ are completely $iid$ covariates
generated according to a $Uniform(0,5)$ distribution. Note that heteroskedasticity is introduced into this model courtesy of $x_{i3}$ affecting the error variance. The fitted model for
this simulation will be a normal linear regression model that has the correct mean structure of an intercept with effects for $x_{i1}$ and $x_{i2}$, but will be fit assuming constant variance.
This will result in unbiased regression parameter estimates; however, these estimates are not guaranteed to be the most efficient, and could be improved by including information regarding $x_{i3}$.

In the final simulation scenario, each $\epsilon_i$ will be generated as $\epsilon_i \stackrel{iid}{\sim} N \left( 0,(2 + 0.5 x_{i2})^2 \right)$, and $x_{i1}$ and $x_{i2}$ are completely $iid$ covariates generated according to
a $Uniform(0,5)$ distribution. The fitted model for this simulation will be a normal linear regression model that has the correct mean structure of an intercept with effects for $x_{i1}$ and $x_{i2}$,
but will be fit assuming homoskedasticity. In this case the heteroskedasticity present is related to a covariate in the model matrix, and thus any normal linear model fit to the data will not be properly
specified if it assumes homoskedasticity, leading to potentially improper inference.

In each simulation, the bootstrap goodness-of-fit test, White test, Breusch-Pagan, classical information matrix test, auxiliary regression information matrix test, and parametric bootstrap information matrix test will each be performed at the $\alpha = .05$ level on the  fitted model. We expect the tests to roughly maintain their
Type~I error rates in the cases where the null hypothesis of each is true, and reveal the power of the test in the simulations where the null hypothesis is violated. After many simulation iterations, the
Type~I error rates or power of each test will be calculated based on the proportion of times each test rejected its null hypothesis. Each simulation will be performed for $n = 50, 100, 250, 500, 750, 1000$ and $2500$, with $1000$
bootstrap iterations for all sets. The simulation will be generate $1000$ samples for each value of $n$, performing the tests each time.

The code for the simulations can be found at \url{https://github.com/shkoeneman/GOF_manuscript/tree/main/scripts}. The results for the first simulation
can be found in the table below.

\begin{table}[H]
	\centering
	
	\small\addtolength{\tabcolsep}{-3pt}
	\setlength\extrarowheight{-3pt}
	\ttabbox[\FBwidth]
	{\caption{\label{tab:sim1_table}Simulation 1 Results - Type~I Error}}
	{
	\resizebox{\textwidth}{!}{%
	\begin{tabular}{ c|c|c|c|c|c|c}
	$n$ & Bootstrap GOF & White & Breusch-Pagan & Classical IM & Regression IM & Bootstrap IM \\
	 \hline
	 50 & 0.038 & 0.041 & 0.042 & 0.598 & 0.766 & 0.027 \\
	 100 & 0.088 & 0.033 & 0.051 & 0.511 & 0.622 & 0.032 \\
	 250 & 0.081 & 0.055 & 0.050 & 0.383 & 0.430 & 0.031 \\
	 500 & 0.078 & 0.050 & 0.057 & 0.238 & 0.260 & 0.034 \\
	 750 & 0.082 & 0.046 & 0.038 & 0.204 & 0.218 & 0.025 \\
	 1000 & 0.089 & 0.058 & 0.050 & 0.182 & 0.187 & 0.035 \\
	 2500 & 0.061 & 0.036 & 0.046 & 0.095 & 0.098 & 0.031 \\
	 \Xhline{3\arrayrulewidth}
	\end{tabular}
	}
	}
\end{table}

The developed bootstrap test exhibits anti-conservative behavior across most values of $n$ with the empirically determined Type~I error rate exceeding the desired Type~I error rate
of $\alpha = 0.05$ in all scenarios. However, as $n$ increases, the empirical Type~I error rate appears to be decreasing towards the desired value, albeit somewhat slowly. Thus, when
using the procedure, one should keep in mind that the test may be more prone to
rejection than one might expect, particularly for small sample sizes before asymptotic properties of the test truly manifest. The parametric information matrix test shows mirrored behavior,
possessing a conservative Type~1 error rate at all values of $n$. The classical and regression variants of the information matrix test fail to maintain a true Type~1 error rate close
to the nominal value until the sample size is rather high, and thus should not be trusted outside of large sizes. The White and Breusch-Pagan tests roughly maintain the desired Type~I error rate across all values of $n$.

The results for the second simulation can be found in the table below.

\begin{table}[H]
	\centering
	\small\addtolength{\tabcolsep}{-3pt}
	\setlength\extrarowheight{-3pt}
	\ttabbox[\FBwidth]
	{\caption{\label{tab:sim2_table}Simulation 2 Results - Power/Type~I Error}}
	{
	\resizebox{\textwidth}{!}{%
	\begin{tabular}{ c|c|c|c|c|c|c}
		$n$ & Bootstrap GOF & White & Breusch-Pagan & Classical IM & Regression IM & Bootstrap IM \\
		 \hline
		 50 & 0.279 & 0.053 & 0.055 & 0.675 & 0.792 & 0.003 \\
		 100 & 0.506 & 0.053 & 0.071 & 0.760 & 0.815 & 0.007 \\
		 250 & 0.828 & 0.065 & 0.050 & 0.860 & 0.881 & 0.009 \\
		 500 & 0.953 & 0.033 & 0.036 & 0.950 & 0.954 & 0.098 \\
		 750 & 0.991 & 0.059 & 0.059 & 0.986 & 0.986 & 0.436 \\
		 1000 & 0.996 & 0.055 & 0.048 & 0.993 & 0.995 & 0.730 \\
		 2500 & 1.000 & 0.053 & 0.042 & 1.000 & 1.000 & 1.000 \\
		 \Xhline{3\arrayrulewidth}
		\end{tabular}
		}
	}
	\end{table}

The power of the bootstrap goodness-of-fit test in this scenario of mean misspecification is rather modest for low values of $n$, but rises rapidly to high levels. Thus, the bootstrap test appears to possess the
ability to detect this mean misspecification on account of an unobserved covariate, and the power to detect this misspecification rises as the sample size increases as one would expect.
The parametric bootstrap variant of the information matrix test eventually reaches similar levels of power, but lags substantially behind until $n$ increases over $1000$. Thus, the bootstrap
goodness-of-fit test appears to possess better power to detect this mean misspecification due to a missing covariate in sample sizes below the highest values employed in the simulation.

The classical and regression-based information matrix tests possess high power for all sample sizes, as is to be expected from their proclivity for rejecting null hypotheses even when they are true.
This characteristic of the classical and regression information matrix tests will be present in the next simulation as well and will not be commented on as such. 

In contrast, the White test and Breusch-Pagan test both seem to only maintain their Type~I error rates established in Simulation 1. This performance was to be expected
as the null hypotheses of these tests are not violated in this scenario. However, this simulation setting does demonstrate how the inability of a goodness-of-fit test to detect improper
fit does not mean lack-of-fit is not present, as these two tests are limited in the scope of misspecification they are able to detect.

The results for the third simulation can be found in the table below.

\begin{table}[H]
	\centering
	\small\addtolength{\tabcolsep}{-3pt}
	\setlength\extrarowheight{-3pt}
	\ttabbox[\FBwidth]
	{\caption{\label{tab:sim3_table}Simulation 3 Results - Power/Type~I Error}}
	{
	\resizebox{\textwidth}{!}{%
	\begin{tabular}{ c|c|c|c|c|c|c}
		$n$ & Bootstrap GOF & White & Breusch-Pagan & Classical IM & Regression IM & Bootstrap IM \\
		 \hline
		 50 & 0.138 & 0.040 & 0.039 & 0.934 & 0.972 & 0.005 \\
		 100 & 0.275 & 0.035 & 0.027 & 0.951 & 0.970 & 0.005 \\
		 250 & 0.498 & 0.058 & 0.040 & 0.988 & 0.991 & 0.049 \\
		 500 & 0.654 & 0.052 & 0.035 & 0.998 & 0.999 & 0.486 \\
		 750 & 0.792 & 0.059 & 0.027 & 0.999 & 0.999 & 0.873 \\
		 1000 & 0.868 & 0.058 & 0.038 & 1.000 & 1.000 & 0.981 \\
		 2500 & 0.996 & 0.115 & 0.036 & 1.000 & 1.000 & 1.000 \\
		 \Xhline{3\arrayrulewidth}
		\end{tabular}
		}
	}
	\end{table}

The bootstrap goodness-of-fit test possesses higher power in the third simulation in small sample sizes than the bootstrap information matrix test. However, at $n = 250$ the bootstrap information matrix test
overtakes the bootstrap goodness-of-fit test, and appears to hold a small lead in power until both tests level out at near perfect power for $n = 2500$. Thus, each test holds an advantage in power for different bands of sample sizes.
The White and Breusch-Pagan tests once again seem to maintain their desired sizes fairly well.

We now move on to the fourth simulation. Figure 1 presents a residual plot generated by fitting the specified model for the fourth simulation setting to data generated as described for this setting.
Note the seemingly patternless, constant variance spread, indicating that one would not find reason to think any assumption of linear regression was violated. 

\begin{figure}[H]
%\includegraphics[angle=-90, scale=0.45]{carat}\par
\includegraphics[width=4.5in]{figures/sim3_residual_plot.pdf}\par
\caption{Simulation 4 Residual Plot.}
\centering
\end{figure}

The results for the fourth simulation can be found in the table below.

\begin{table}[H]
	\centering
	\small\addtolength{\tabcolsep}{-3pt}
	\setlength\extrarowheight{-3pt}
	\ttabbox[\FBwidth]
	{\caption{\label{tab:sim4_table}Simulation 4 Results - Power/Type~I Error}}
	{
	\resizebox{\textwidth}{!}{%
	\begin{tabular}{ c|c|c|c|c|c|c}
		$n$ & Bootstrap GOF & White & Breusch-Pagan & Classical IM & Regression IM & Bootstrap IM \\
		 \hline
		 50 & 0.006 & 0.054 & 0.046 & 0.051 & 0.702 & 0.146 \\
		 100 & 0.121 & 0.048 & 0.038 & 0.356 & 0.525 & 0.307 \\
		 250 & 0.506 & 0.047 & 0.036 & 0.183 & 0.317 & 0.538 \\
		 500 & 0.891 & 0.044 & 0.041 & 0.160 & 0.369 & 0.790 \\
		 750 & 0.976 & 0.052 & 0.049 & 0.335 & 0.547 & 0.928 \\
		 1000 & 0.994 & 0.062 & 0.055 & 0.547 & 0.692 & 0.972 \\
		 2500 & 1.000 & 0.057 & 0.062 & 0.994 & 0.994 & 1.000 \\
		 \Xhline{3\arrayrulewidth}
		\end{tabular}
		}
	}
	\end{table}

The bootstrap goodness-of-fit test appears to require a relatively high sample size to produce adequate power. However, the test does seem to eventually be able to detect this violation of homoskedasticity.
The bootstrap information matrix test performs somewhat similarly, possessing higher power for small sample sizes and lower power for moderate sample sizes. The classic and regression variants of the information
matrix test display erratic power as $n$ increases, and thus further show their unreliability.

The White test and Breusch-Pagan test exhibit only power in line with their Type~I error rates established earlier, and thus do not seem to be able to detect this form of heteroskedasticity,
as the residual plot could not. This reveals how the subtleness of this homoskedasticity, which should not affect bias of estimates, but may affect efficiency.

This limitation in detection is on account of how the Breusch-Pagan and White tests and residual plots are constructed, as they rely on the detection of heteroskedasticity that is induced by observed covariates.
When heteroskedasticity is induced by an unobserved covariate that is not used in the model, the White test and Breusch-Pagan test cannot ascertain that their
null hypotheses of homoskedasticity are violated, and residual plots do not show any visible heteroskedasticity. However, the bootstrap goodness-of-fit test and variants of the information matrix test are able to
reject their null hypotheses.

The results for the fifth and final simulation can be found in the table below. Note that the null hypotheses of all tests are violated in this case, and each test should be able to detect this form of misspecification.

\begin{table}[H]
	\centering
	\small\addtolength{\tabcolsep}{-3pt}
	\setlength\extrarowheight{-3pt}
	\ttabbox[\FBwidth]
	{\caption{\label{tab:sim5_table}Simulation 5 Results - Power}}
	{
	\resizebox{\textwidth}{!}{%
	\begin{tabular}{ c|c|c|c|c|c|c}
		$n$ & Bootstrap GOF & White & Breusch-Pagan & Classical IM & Regression IM & Bootstrap IM \\
		 \hline
		 50 & 0.018 & 0.169 & 0.327 & 0.725 & 0.875 & 0.146 \\
		 100 & 0.036 & 0.314 & 0.695 & 0.855 & 0.912 & 0.307 \\
		 250 & 0.141 & 0.773 & 0.992 & 0.988 & 0.992 & 0.538 \\
		 500 & 0.413 & 0.995 & 1.000 & 1.000 & 1.000 & 0.790 \\
		 750 & 0.583 & 1.000 & 1.000 & 1.000 & 1.000 & 0.928 \\
		 1000 & 0.732 & 1.000 & 0.055 & 1.000 & 1.000 & 0.972 \\
		 2500 & 0.991 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 \\
		 \Xhline{3\arrayrulewidth}
		\end{tabular}
		}
	}
	\end{table}

In this scenario, the White test and Breusch-Pagan test both perform well in moderate to large sample sizes. This is likely due to the heteroskedasticity being induced by a covariate that is also in the 
proposed mean structure, the very case for which the White and Breusch-Pagan tests were designed. The bootstrap variant of the information matrix test performs similarly, while the classical and regression
variants again display extreme power at low sample sizes. While the bootstrap goodness-of-fit test is able to detect this misspecification, it requires a very large sample size to have adequate power.

In summary, we see that the new bootstrap goodness-of-fit test possesses advantages over competetitor tests, particularly in its ability to detect multiple forms of misspecification and ability to detect
forms of mean misspecification where an important covariate may be missing.

\section{Discussion and Conclusion}

In this work, we have developed a new bootstrap-based goodness-of-fit procedure to assess the goodness-of-fit of a fitted normal linear regression model. In order
to develop this test, we first derived an asymptotic variance for the goodness-of-fit term present in likelihood-based information criteria such as AIC under the assumption
that the fitted regression model is properly specified. The test functions by assessing whether a robust estimated variance conforms to the
theoretical value under proper specification. Simulation studies demonstrated the ability of this bootstrap test to detect a wide variety of violations of assumptions inherent to
normal linear regression. These violations include mean misspecification and violations to homoskedasticity. As such, this new procedure has the potential to serve as an omnibus
goodness-of-fit procedure for normal linear regression models. Additionally, we developed the \texttt{BootGOFTestLM} function of the R package \texttt{DBModelSelect} to provide
an efficient and convenient method of performing the test.

This procedure has the potential to assist in model selection. For example, if one were to consider a normal linear regression framework to model a certain outcome, one may
choose to employ the bootstrap test on the largest possible model in consideration, as if this model is misspecified, then all nested models will also be misspecified.
If the test does not reject its null hypothesis of proper specification, one could proceed with the normal linear regression framework and select a final model using
an information criterion such as Mallows' Cp, or by employing another method such as cross validation.

This bootstrap goodness-of-fit procedure is not without its drawbacks. While the test is able to detect heteroskedasticity induced by unobserved covariates, this may be viewed as a detriment,
as this form of heteroskedasticity will not affect bias, and only efficiency. Without knowledge of unobserved covariates, one may not be able to improve efficiency. Thus, if the test rejects its
null hypothesis but one still would like  to move forward with a normal linear regression model, it is recommended to use robust estimates for the variance of estimated effects
to hedge against performing improper inference. Simulations showed that the bootstrap test may be underpowered for small sample sizes. A bootstrap rendition of the information matrix test may
possess better power to detect certain forms of misspecification. Additionally, the test may be overpowered for real-life datasets with large sample sizes, resulting in rejection of the null hypothesis
when in fact a linear regression is a reasonable model. This is a feature common to many goodness-of-fit tests, and must be remembered when performing an analysis on observed data.

Potential future work involves finding similar procedures that can be used for other modeling frameworks.

%%% Supplementary materials (if any)
%%% ------------------------------------------
%\section*{Supplementary materials}
%The supplementary materials include [the list of supplementary materials and their brief description]. They are available from the journals page at \url{https://journals.sagepub.com/home/smj}. [At the submission stage, all supplementary materials (or the links to access them) should be submitted jointly with the main paper.]


%%% Acknowledgements (if any)
%%% ------------------------------------------
%\section*{Acknowledgements}
%We want to thank\ldots 


%%% Declaration of conflicting interests (should always be included)
%%% -----------------------------------------------------------------
\section*{Declaration of conflicting interests}
The authors declared no potential conflicts of interest with respect to the research, authorship and/or
publication of this article.
   %%% Alternatively, please, disclose here potential conflicting interests. 


%%% Funding (if any)
%%% ------------------------------------------
%\section*{Funding}
%This is the place to mention the funding if it is applicable for the paper.


%%% Appendix (if any)
%%% ------------------------------------------
\appendix
\section*{Appendix}
This Appendix will derive an exact variance for the goodness-of-fit term $-2 \ell (\hat{\theta} )$ in the case of a properly specified
linear model. While a similar derivation has been presented in other work \citep{McQuarrie}, the following derivation differs substantially and is presented for completeness.
We assume that a given linear regression model has a full rank $n$ by $r$ design matrix $X$, outcome vector $y$, true parameters $\beta$ and $\sigma^2$,
and maximum likelihood estimates $\hat{\beta}$ and $\hat{\sigma}^2$. We let
\begin{equation*}
	A = \frac{1}{\sigma^2} (I_n - X(X'X)^{-1}X') 
\end{equation*}
such that we can construct the quadratic form
\begin{equation*}
	y'Ay = \frac{n \hat{\sigma}^2}{\sigma^2} .
\end{equation*}
By the properties of quadratic forms of multivariate normal random variables, it can be seen that
\begin{equation}
	y'Ay = \frac{n \hat{\sigma}^2}{\sigma^2} \sim \chi^2_{n-r} .
\end{equation}
Note that if we take the natural logarithm of the left-hand side and evaluate its variance, we see that
\begin{equation}
	Var \left[ \log(\frac{n \hat{\sigma}^2}{\sigma^2}) \right]  = Var \left[ \log \hat{\sigma}^2 \right]
\end{equation}
due to constant terms being irrelevant to the variance calculation. This enclosed form is very close to the $n \log \hat{\sigma}^2$ that is required
to determine the variance of the likelihood goodness of fit term $-2 \ell (\hat{\theta} )$ in the case of a normal linear regression model.
Thus, if we can calculate $Var \left[ \log \hat{\sigma}^2 \right]$, which is itself the variance
of the natural logarithm of a variable distributed as $\chi^2_{n-r}$, we can easily obtain $Var \left[ n \log \hat{\sigma}^2 \right]$.

Let $W \sim \chi^2_{\nu}$, and let $Y = \log(W)$. Note that the moment generating function of $Y$ can be expressed as
\begin{equation*}
	M_Y (t) = E \left[ e^{yt} \right] = E \left[ e^{t\log(w)} \right] = E \left[ e^{\log w^t} \right] = E \left[ w^t \right] .
\end{equation*}
Thus, the moment generating function of $Y$ evaluated at $t$ is the $t^{th}$ moment of $W$. The moments of $W$ will have the
closed form of
\begin{equation*}
	E \left[ w^t \right] = 2^t \frac{\Gamma (t + \frac{\nu}{2})}{\Gamma (\frac{\nu}{2})} = E \left[ e^{yt} \right] = M_Y (t) .
\end{equation*}

Taking the first derivative with respect to $t$ of the moment generating function $M_Y (t)$ and evaluating at $t=0$, we find the first moment of $Y$ to be
\begin{equation*}
	E \left[ Y^1 \right] =  \log 2 + \psi^{(0)} \left( \frac{\nu}{2} \right) ,
\end{equation*}
where $\psi^{(0)}(z)$ is the digamma function. Thus, we have a form for the first moment of the logarithm of a central chi-squared random variable. By taking another
derivative with respect to $t$ and evaluating at $t=0$, we find the second moment to be
\begin{equation*}
	E \left[ Y^2 \right] =
	(\log 2)^2 + \psi^{(0)} \left( \frac{\nu}{2} \right) 2 \log 2  + \psi^{(1)} \left( \frac{\nu}{2} \right) + \left( \psi^{(0)} \left( \frac{\nu}{2} \right) \right)^2 ,
\end{equation*}
where $\psi^{(1)}(z)$ is the digamma function.

Therefore, the variance of $Y$, the log of a central chi-squared random variable with
$\nu$ degrees of freedom, is found to be
\begin{equation*}
	Var \left[ Y \right] = E \left[ Y^2 \right] - \left( E \left[ Y \right] \right)^2 = \psi^{(1)} \left( \frac{\nu}{2} \right) .
\end{equation*}
This value can be calculated using software to approximate the trigamma function, although it should be noted that this calculation may be unstable for certain values of the degrees of freedom.

Recalling the distributional result established in (4.3) and the relation in (4.4), it is clear that
\begin{equation*}
	Var \left[ n \log \hat{\sigma}^2 \right] = Var \left[ -2 \ell (\hat{\theta} ) \right] = n^2 \psi^{(1)} \left( \frac{n-r}{2} \right).
\end{equation*}

However, while this variance is exact, it still involves an approximation as the trigamma function must be approximated. Additionally, it was found that
this variance provided no benefit when used in the bootstrap goodness-of-fit procedure developed in this work. Thus, this derivation is presented here only
for completeness and as a matter of theoretical interest.

%%% References if bibTeX is used
%%%
%%% Please, do not specify any \bibliographystyle{} command!
%%%
%%% It is already specified in the smj.cls and its
%%% second specification here causes error.
%%% ------------------------------------------------------------
\bibliography{GOF_main_SS}


%%% References (if created by hand).
%%% -----------------------------------------------------------------------------------

%\bibliographystyle{plain}

\iffalse

\begin{thebibliography}{99}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Akaike(1974)]{Akaike}
{\rm Akaike, H.} (1974).
\newblock A new look at the statistical identification model.
\newblock \emph{IEEE Transactions on Automatic Control}, {\bf 19}, \penalty0 716--723.

\bibitem[Breusch and Pagan(1989)]{Breusch}
{\rm Breusch, T.S., and Pagan, A.R.} (1989).
\newblock A Simple Test for Heteroskedasticity and Random Coefficient Variation.
\newblock \emph{Econometrica}, {\bf 47}, \penalty0 1287--1294.

\bibitem[Cavanaugh and Neath(2019)]{Cavanaugh}
{\rm Cavanaugh, J.E., and Neath, A.A.} (2019).
\newblock The Akaike information criterion: Background, derivation, properties, application, interpretation, and refinements.
\newblock \emph{WIREs Comput Stat}, 11:e1460.

\bibitem[Chesher(1983)]{Chesher}
{\rm Chesher, A.} (1983).
\newblock The Information Matrix Test: Simplified Calculation via a Score Test Interpretation.
\newblock \emph{Economics Letters}, {\bf 13}, \penalty0 45--48.

\bibitem[Dhaene and Hoorelbeke(2004)]{Dhaene}
{\rm Dhaene, G., and Hoorelbeke, D.} (2004).
\newblock The information matrix test with bootstrap-based covariance matrix estimation.
\newblock \emph{Economics Letters}, {\bf 82}, \penalty0 341--347.

\bibitem[Fisher(1922)]{Fisher}
{\rm Fisher, R.A.} (1922).
\newblock On the Mathematical Foundations of Theoretical Statistics.
\newblock \emph{Phil. Trans. R. Soc. Lond. A}, {\bf 222}, \penalty0 309--368.

\bibitem[Freedman(2006)]{Freedman}
{\rm Freedman, D.A.} (2006).
\newblock On The So-Called 'Huber Sandwich Estimator' and 'Robust Standard Errors'.
\newblock \emph{The American Statistician}, {\bf 60}, \penalty0 299--302.

\bibitem[Huber(1967)]{Huber}
{\rm Huber, P.J.} (1967).
\newblock The behavior of maximum likelihood estimates under nonstandard conditions.
\newblock \emph{Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability}, {\bf 5}, \penalty0 221--233.

\bibitem[Koeneman(2023)]{Koeneman}
{\rm Koeneman, S.H.} (2023).
\newblock \emph{DBModelSelect: Distribution-Based Model Selection}.
\newblock R package version 0.2.0, \url{https://CRAN.R-project.org/package=DBModelSelect}.

\bibitem[Kutner et al.(2005)]{Kutner}
{\rm Kutner, M.H., Nachtsheim, C.R., Neter, J., and Li, W.} (2005).
 \emph{Applied Linear Statistical Models}.
\newblock McGraw-Hill Irwin, New York.

\bibitem[McQuarrie and Tsai(1998)]{McQuarrie}
{\rm McQuarrie, A.D.R., and Tsai, C-L} (1998).
 \emph{Regression and Time Series Model Selection.}
\newblock World Scientific, New Jersey.

\bibitem[Miles(2014)]{Miles}
{\rm Miles, J.} (2014).
\newblock Residual plot.
\newblock \emph{Wiley StatsRef: Statistics Reference Online}.

\bibitem[Millar(2011)]{Millar}
{\rm Millar, R.B.} (2011).
\newblock \emph{Maximum Likelihood Estimation and Inference: With Examples in R, SAS and ADMB.}
\newblock Wiley, Hoboken.

\bibitem[Rao(1965)]{Rao}
{\rm Rao, C.R.} (1965).
\newblock \emph{Linear Statistical Inference and Its Applications.}
\newblock Wiley, Hoboken.

\bibitem[Schwarz(1978)]{Schwarz}
{\rm Schwarz, G.E.} (1978).
\newblock Estimating the dimension of a model.
\newblock \emph{Annals of Statistics}, {\bf 22}, \penalty0 461--464.

\bibitem[White(1980)]{White1980}
{\rm White, H.} (1980).
\newblock A Heteroskedasticity-Consistent Covariance Matrix Estimator and a Direct Test for Heteroskedasticity.
\newblock \emph{Econometrica}, {\bf 48}, \penalty0 817--838.

\bibitem[White(1982)]{White1982}
{\rm White, H.} (1982).
\newblock Maximum Likelihood Estimation of Misspecified Models.
\newblock \emph{Econometrica}, {\bf 50}, \penalty0 1--25.

\bibitem[Waldman(1983)]{Waldman}
{\rm Waldman, D.M.} (1983).
\newblock A note on algebraic equivalence of White's test and a variation of the Godfrey/Breusch-Pagan test for heteroscedasticity.
\newblock \emph{Economics Letters}, {\bf 13}, \penalty0 197--200.

\end{thebibliography}

\fi

%\bibitem[Fahrmeir et~al.(2013)Fahrmeir, Kneib, Lang, and Marx]{Fahrmeiret13}
%{\rm Fahrmeir, L., Kneib, T., Lang, S., {\rm and} Marx, B.} (2013).
%\newblock \emph{Regression: Models, Methods and Applications}.
%\newblock Springer-Verlag, New York.
%
%\bibitem[G\'omez et~al.(2009)G\'omez, Luz~Calle, Oller, and Langohr]{Gomezet09}
%{\rm G\'omez, G., Luz~Calle, M., Oller, R., {\rm and} Langohr, K.} (2009).
%\newblock Tutorial on methods for interval-censored data and their implementation in {R}.
%\newblock \emph{Statistical Modelling}, {\bf 9}\penalty0 (4), \penalty0 259--297.
%\newblock \doi{10.1177/1471082X0900900402}.
%
%\bibitem[Kneib(2013)]{Kneib13}
%{\rm Kneib, T.} (2013).
%\newblock Beyond mean regression.
%\newblock \emph{Statistical Modelling}, {\bf 13}\penalty0 (4), \penalty0 275--303.
%\newblock \doi{10.1177/1471082X13494159}.
%
%\bibitem[Kom\'arek and Lesaffre(2006)]{KomarekLesaffre06}
%{\rm Kom\'arek, A. {\rm and} Lesaffre, E.} (2006).
%\newblock Bayesian semi-parametric accelerated failure time model for paired doubly-interval-censored data.
%\newblock \emph{Statistical Modelling}, {\bf 6}\penalty0 (1), \penalty0 3--22.
%\newblock \doi{10.1191/1471082X06st107oa}.
%
%\bibitem[Lesaffre et~al.(2009)Lesaffre, Kom\'arek, and Jara]{Lesaffreet09}
%{\rm Lesaffre, E., Kom\'arek, A., {\rm and} Jara, A.} (2009).
%\newblock The {B}ayesian approach.
%\newblock In {\rm Lesaffre, E., Feine, J., Leroux, B., {\rm and} Declerck, D.}, editors,
%  \emph{Statistical and Methodological Aspects of Oral Health Research},
%  pages 315--338. John Wiley and Sons, Chichester.
%
%\bibitem[Li et~al.(2007)Li, Simonoff, and Tsai]{Liet07}
%{\rm Li, L., Simonoff, J.~S., {\rm and} Tsai, C.-L.} (2007).
%\newblock Tobit model estimation and sliced inverse regression.
%\newblock \emph{Statistical Modelling}, {\bf 7}\penalty0 (2), \penalty0 107--123.
%\newblock \doi{10.1177/1471082X0700700201}.
%
%\bibitem[Waldmann et~al.(2013)Waldmann, Kneib, Yue, Lang, and Flexeder]{Waldmannet13}
%{\rm Waldmann, E., Kneib, T., Yue, Y.~R., Lang, S., {\rm and} Flexeder, C.} (2013).
%\newblock Bayesian semiparametric additive quantile regression.
%\newblock \emph{Statistical Modelling}, {\bf 13}\penalty0 (3), \penalty0 223--252.
%\newblock \doi{10.1177/1471082X13480650}.
%


\end{document}
